<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <title> David Lindell </title>
        <meta charset="utf-8"> 
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="shortcut icon" type="image/x-icon" href="/im/favicon.ico">
        <link href="https://fonts.googleapis.com/css?family=Istok+Web" rel="stylesheet">
        <link href="css/lindell.css" rel="stylesheet">
        <link href="vendor/css/bootstrap.min.css" rel="stylesheet">
        <link href="vendor/css/bootstrap-theme.min.css" rel="stylesheet">
        <link href="vendor/css/academicons.min.css" rel="stylesheet">
        <link href="vendor/css/font-awesome.min.css" rel="stylesheet">

    </head>
    <body>

      
        <nav class="navbar navbar-default navbar-fixed-top" id="site_nav">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/" style="font-size:24px">David Lindell</a>
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse" id="navbar-collapse-1">
                    <ul class="nav navbar-nav">
                        <li><a href="#news">News <span class="sr-only">(current)</span></a></li>
                        <li><a href="#education">Education </a></li>
                        <li><a href="#research">Research </a></li>
                        <li><a href="#publications">Publications </a></li>
                        <li><a href="#projects">Projects </a></li>
                        <li><a href="#experience">Experience </a></li>
                    </ul>
                    
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="http://github.com/davelindell" target="_blank">
                                <i class="fa fa-lg fa-github" aria-hidden="true"></i>
                            </a>
                        </li>
                        <li>
                            <a href="https://scholar.google.com/citations?user=_m-BTtAAAAAJ&hl=en" target="_blank">
                                <i class="ai ai-lg ai-google-scholar" aria-hidden="true"></i>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.linkedin.com/in/david-lindell-a434b882" target="_blank">
                                <i class="fa fa-lg fa-linkedin" aria-hidden="true"></i>
                            </a>
                        </li>
                    </ul>
                </div><!-- /.navbar-collapse -->

            </div><!-- /.container-fluid -->
        </nav>

        <div class = "container">
            <div class="row">
                <div class="col-md-5">
                    <div style='font-size: 5em; font-weight: bold; padding-bottom: 0.0em;'> David Lindell</div>

                    <div style='font-size: 2em; font-weight: bold; padding-bottom: 0.3em;'> 
                        Ph.D. Student
                    </div>
                    <div style='font-size: 2em; font-weight: bold; padding-bottom: 0.3em;'> 
                        <a href="http://stanford.edu"> Stanford University </a>
                    </div>
                    <div style='font-size: 2em; font-weight: bold; padding-bottom: 0.3em;'> 
                        <a href="http://ee.stanford.edu"> Electrical Engineering</a>
                    </div>

                    <a href="/data/cv.pdf" target="_blank" class="btn btn-primary" style="padding: 0.5em; margin-top:-20px; margin-right:20px;">
                        <i class="fa fa-download"></i> Download CV
                    </a>
                    <a href="http://github.com/davelindell" target="_blank" style="margin:20px">
                        <i class="fa fa-3x fa-github" aria-hidden="true"></i>
                    </a>
                    <a href="https://scholar.google.com/citations?user=_m-BTtAAAAAJ&hl=en" target="_blank" style="margin:20px">
                        <i class="ai ai-3x ai-google-scholar" aria-hidden="true"></i>
                    </a>
                    <a href="https://www.linkedin.com/in/david-lindell-a434b882" target="_blank" style="margin:20px">
                        <i class="fa fa-3x fa-linkedin" aria-hidden="true"></i>
                    </a>



                </div>

                <div class="col-md-7">
                    <div style='font-size: 5em; font-weight: bold; padding-bottom: 0.3em;'> 
                        <a href="im/lindell2.jpg">
                            <img src="im/lindell2.jpg"
                              width="500" style="border-radius: 10px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 0px"
                              alt="David Lindell"/>
                        </a>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-md-11" style='font-size:1.5em'>
                    I'm a second-year Ph.D. student at Stanford University. I completed my B.S. and M.S. degrees in Electrical Engineering at Brigham Young University and am interested in problems in imaging, optimization, computer vision, and remote sensing. I'm currently working in <a href="http://www.computationalimaging.org/"> Stanford's Computational Imaging Lab </a> on time-of-flight sensors, imaging around corners, and next-generation LIDAR systems. <strong></strong>
                </div>  
            </div>

            <h1 id="news">News</h1>
            <div class="row">
            <table class="table table-hover">
                <tr>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                           June 2018 
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style="font-size:1.5em;">
                            I'm interning at the Intelligent Systems Lab at Intel this summer with <a href="http://vladlen.info">Vladlen Koltun</a>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                           March 2018 
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em;'>
                            Our paper on seeing around corners was published in <a href="http://rdcu.be/ImAZ">Nature</a>!
                        </div>
                    </td>
                </tr>
            </table>
            </div>

            <h1 id="education">Education</h1>
            <div class="row">
            <table class="table table-hover">

                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href=""> 
                                <img src="im/su_logo.png"
                                  width="95" style="border-radius: 33px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 0px"
                                  alt="stanford"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 30px;"> 
                            2016-Present
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong> Stanford University </strong> <br />
                            Ph.D Electrical Engineering
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href="http://home.byu.edu/home/"> 
                                <img src="im/byu_logo.jpg"
                                  width="95" style="border-radius: 39px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 0px"
                                  alt="byu"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 20px;"> 
                            2009-2016
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong> Brigham Young University </strong> <br />
                            B.S. Electrical Engineering <i>Summa Cum Laude</i> <br />
                            M.S. Electrical Engineering 
                        </div>
                    </td>
                </tr>
            </table>
            </div>


            <h1 id="research">Research</h1>
            <div class="row">
            <table class="table table-hover">

                <tr>
                    <td class="col-md-5" > 
                        <div style="font-size:1.5em;"> 
                            2016-Present
                        </div>
                    </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong> Stanford University</strong>, Ph.D. Student <br />
                            <i>Advisor:</i> Gordon Wetzstein <br />
                            <i>Area:</i> Computational Imaging, single photon detectors, virtual reality <br />
                            <i>Project:</i> Reconstruction of transient images, non-line of sight imaging, virtual reality rendering
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-5" > 
                        <div style="font-size:1.5em;"> 
                            2014-2016
                        </div>
                    </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong> Brigham Young University</strong>, M.S. Student <br />
                            <i>Advisor:</i> David Long <br />
                            <i>Area:</i> Radar Image Processing, Resolution Enhancement, Geoscience <br />
                            <i>Project:</i> Arctic ice classification, soil moisture estimation from C-band/Ku-band scatterometers and radiometers
                        </div>
                    </td>
                </tr>
                
                <tr>
                    <td class="col-md-5" > 
                        <div style="font-size:1.5em;"> 
                            2013-2014
                        </div>
                    </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong> Brigham Young University</strong>, B.S. Student <br />
                            <i>Advisor:</i> Aaron Hawkins <br />
                            <i>Area:</i> Semiconductor Devices, Cleanroom Fabrication, Circuit Design  <br />
                            <i>Project:</i> Fabrication of a solid-state single ion detection unit
                        </div>
                    </td>
                </tr>
            </table>
            </div>

            <h1 id="publications">Publications</h1>
            <div class="row">
            <table class="table table-hover">

                <tr>
                    <td class="col-md-3"><a href="http://www.computationalimaging.org/publications/single-photon-3d-imaging-with-deep-sensor-fusion/" target="_blank"><img width=300 src="im/siggraph_2018.jpg"/></a> </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong>D. B. Lindell</strong>, M. O'Toole, G. Wetzstein, “Single-Photon 3D Imaging with Deep Sensor Fusion”, <i>ACM SIGGRAPH</i>, 2018.
                        </div>
                        <div style='font-size:1.5em'>
                            <a style="cursor: pointer;" class='flip' onclick="$(this).next('.panel').slideToggle();"> [Abstract]</a>
                            <a style="cursor: pointer;" href="http://www.computationalimaging.org/publications/single-photon-3d-imaging-with-deep-sensor-fusion/"> [Webpage]</a>
                            <a style="cursor: pointer;" href="https://drive.google.com/a/stanford.edu/file/d/1eTIawad7GFob3p5nuxrDcO7FMtL3AzSt/view?usp=sharing"> [PDF]</a>
                            <a style="cursor: pointer;" href="https://drive.google.com/a/stanford.edu/file/d/1DtFQKKaPoP4Btj1Nv6DowuWC_JBBa-cT/view?usp=sharing"> [Supplement]</a>
                            <a style="cursor: pointer;" href="https://www.youtube.com/watch?v=dg_73m4e_Js"> [Video]</a>
                        </div>
                        <div class="panel" style="display:none; font-size:18px; text-align: justify;">
                            <p>Sensors which capture 3D scene information provide useful data for tasks in vehicle navigation, gesture recognition, human pose estimation, and geometric reconstruction. Active illumination time-of-flight sensors in particular have become widely used to estimate a 3D representation of a scene. However, the maximum range, density of acquired spatial samples, or overall acquisition time of these sensors is fundamentally limited by the minimum signal required to estimate depth reliably. In this paper, we propose a data-driven method for photon-efficient 3D imaging which leverages sensor fusion and computational reconstruction to rapidly and robustly estimate a dense depth map from low photon counts. Our sensor fusion approach uses measurements of single photon arrival times from a low-resolution single-photon detector array and an intensity image from a conventional high-resolution camera. Using a multi-scale deep convolutional network, we jointly process the raw measurements from both sensors and output a high-resolution depth map. To demonstrate the efficacy of our approach, we implement a hardware prototype and show results using captured data. At low signal-to-background levels our depth reconstruction algorithm with sensor fusion outperforms other methods for depth estimation from noisy measurements of photon arrival times.</p>
                        </div>
                    </td>
                </tr>


                <tr>
                    <td class="col-md-3"><a href="http://www.computationalimaging.org/publications/reconstructing-transient-images-from-single-photon-sensors-cvpr-2017/" target="_blank"><img width=300 src="im/iccp_2018.png"/></a> </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong>D. B. Lindell</strong>, M. O'Toole, G. Wetzstein, “Towards Transient Imaging at Interactive Rates with Single-Photon Detectors”, <i>Proc. ICCP</i>, 2018.
                        </div>
                        <div style='font-size:1.5em'>
                            <a style="cursor: pointer;" class='flip' onclick="$(this).next('.panel').slideToggle();"> [Abstract]</a>
                            <a style="cursor: pointer;" href="http://www.computationalimaging.org/publications/reconstructing-transient-images-from-single-photon-sensors-cvpr-2017/"> [Webpage]</a>
                            <a style="cursor: pointer;" href="https://drive.google.com/a/stanford.edu/file/d/1b2Ls8bUghMl2h8gZMnhgfSYbbRHb83RD/view?usp=sharing"> [PDF]</a>
                            <a style="cursor: pointer;" href="https://www.youtube.com/watch?v=i6Z3NDgXLec"> [Video]</a>
                            <a style="cursor: pointer;" href="https://www.youtube.com/watch?time_continue=2607&v=pgZ3HiNaF4k"> [Conference Presentation]</a>
                        </div>
                        <div class="panel" style="display:none; font-size:18px; text-align: justify;">
                            <p>Active imaging at the picosecond timescale reveals transient light transport effects otherwise not accessible by computer vision and image processing algorithms. For example, analyzing the time of flight of short laser pulses emitted into a scene and scattered back to a detector allows for depth imaging, which is crucial for autonomous driving and many other applications. Moreover, analyzing or removing global light transport effects from photographs becomes feasible.</p> 

                            <p>While several transient imaging systems have recently been proposed using various imaging technologies, none is capable of acquiring transient images at interactive framerates. In this paper, we present an imaging system that records transient images at up to 25 Hz. We show several transient video clips recorded with this system and demonstrate transient imaging applications, including direct-global light transport separation and enhanced depth imaging.
</p>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-3"><a href="http://www.computationalimaging.org/publications/confocal-non-line-of-sight-imaging-based-on-the-light-cone-transform/" target="_blank"><img width=300 src="im/nature_teaser.png"/></a> </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            M. O’Toole, <strong>D. B. Lindell</strong>, G. Wetzstein, “Confocal Non-Line-of-Sight Imaging based on the Light Cone Transform”, <i>Nature</i>, 2018.
                        </div>
                        <div style='font-size:1.5em'>
                            <a style="cursor: pointer;" class='flip' onclick="$(this).next('.panel').slideToggle();"> [Abstract]</a>
                            <a style="cursor: pointer;" href="http://www.computationalimaging.org/publications/confocal-non-line-of-sight-imaging-based-on-the-light-cone-transform/"> [Webpage]</a>
                            <a style="cursor: pointer;" href="http://rdcu.be/ImAZ"> [PDF]</a>
                            <a style="cursor: pointer;" href="https://www.nature.com/articles/nature25489#supplementary-information"> [Supplement]</a>
                        </div>
                        <div class="panel" style="display:none; font-size:18px; text-align: justify;">
                            <p>Imaging objects hidden from a camera’s view is a problem of fundamental importance to many fields of research with applications in robotic vision, defense, remote sensing, medical imaging, and autonomous vehicles. Non-line-of-sight (NLOS) imaging at macroscopic scales has been demonstrated by scanning a visible surface with a pulsed laser and time-resolved detector. Whereas light detection and ranging (LIDAR) systems use such measurements to recover the shape of visible objects from direct reflections, NLOS imaging aims at reconstructing the shape and albedo of hidden objects from multiply scattered light. Despite recent advances, NLOS imaging has remained impractical due to the prohibitive memory and processing requirements of existing reconstruction algorithms, and the extremely weak signal of multiply scattered light. Here we show that confocalizing the scanning procedure provides a means to address these key challenges. Confocal scanning facilitates the derivation of a novel closed-form solution to the NLOS reconstruction problem, which requires computational and memory resources that are orders of magnitude fewer than previous reconstruction methods and recovers hidden objects at unprecedented image resolutions. Confocal scanning also uniquely benefits from a sizeable increase in signal and range when imaging retroreflective objects. We quantify the resolution bounds of NLOS imaging, demonstrate real-time tracking capabilities, and derive efficient algorithms that incorporate image priors and a physically-accurate noise model. Most notably, we demonstrate successful outdoor experiments for NLOS imaging under indirect sunlight.</p>
                        </div>
                        <div style='font-size:1.5em'>
                            <p>
                            <a href="https://www.youtube.com/watch?v=KnGQEzB9u_0" target="_blank"><img width=200 src="im/press/stanford-news-service-logo.png" style="margin:8px"/>
                            <a href="https://www.wired.com/story/wanna-see-around-corners-better-get-yourself-a-laser/" target="_blank"><img width=200 src="im/press/wired.png" style="margin:8px"/>
                            <a href="https://www.theguardian.com/technology/2018/mar/05/self-driving-cars-may-soon-be-able-to-see-around-corners" target="_blank"><img width=200 src="im/press/guardian.png" style="margin:8px"/>
                            </p>
                        </div>
                    </td>
                </tr>
 
                <tr>
                    <td class="col-md-3"><a href="http://www.computationalimaging.org/publications/reconstructing-transient-images-from-single-photon-sensors-cvpr-2017/" target="_blank"><img width=300 src="im/cvpr17.png" /></a> </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            M. O’Toole, F. Heide, <strong>D. B. Lindell</strong>, K. Zang, S. Diamond, G. Wetzstein, “Reconstructing Transient Images from Single-Photon Sensors”, <i>Proc. CVPR</i>, 2017.
                        </div>
                        <div style='font-size:1.5em'>
                            <a style="cursor: pointer;" class='flip' onclick="$(this).next('.panel').slideToggle();"> [Abstract]</a>
                            <a style="cursor: pointer;" href="http://www.computationalimaging.org/publications/reconstructing-transient-images-from-single-photon-sensors-cvpr-2017/"> [Webpage]</a>
                            <a style="cursor: pointer;" href="http://www.computationalimaging.org/wp-content/uploads/2017/05/17.cvpr_.spad_.pdf"> [PDF]</a>
                            <a style="cursor: pointer;" href="http://www.computationalimaging.org/wp-content/uploads/2017/04/17.cvpr_.spad_.supp_.pdf"> [Supplement]</a>
                        </div>
                        <div class="panel" style="display:none; font-size:18px; text-align: justify;">
                            <p>Computer vision algorithms build on 2D images or 3D videos that capture dynamic events at the millisecond time scale. However, capturing and analyzing “transient images” at the picosecond scale—i.e., at one trillion frames per second—reveals unprecedented information about a scene and light transport within. This is not only crucial for time-of-flight range imaging, but it also helps further our understanding of light transport phenomena at a more fundamental level and potentially allows to revisit many assumptions made in different computer vision algorithms.</p>

                            <p>In this work, we design and evaluate an imaging system that builds on single photon avalanche diode (SPAD) sensors to capture multi-path responses with picosecond-scale active illumination. We develop inverse methods that use modern approaches to deconvolve and denoise measurements in the presence of Poisson noise, and compute transient images at a higher quality than previously reported. The small form factor, fast acquisition rates, and relatively low cost of our system potentially makes transient imaging more practical for a range of applications.</p>
                        </div>
                    </td>
                </tr>
 
                <tr>
                    <td class="col-md-3"><a href="http://ieeexplore.ieee.org/document/7172497/?arnumber=7172497" target="_blank"><img width=300 src="im/oscat.png" /></a> </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong>D. B. Lindell </strong> and D. G. Long, "Multiyear Arctic Sea Ice Classification Using OSCAT and QuikSCAT," 
                            in <i>IEEE Transactions on Geoscience and Remote Sensing</i>, vol. 54, no. 1, pp. 167-175, Jan. 2016.
                            doi: <a href="http://dx.doi.org/10.1109/TGRS.2015.2452215">10.1109/TGRS.2015.2452215</a>
                        </div>
                        <div style='font-size:1.5em'>
                            <a style="cursor: pointer;" class='flip' onclick="$(this).next().slideToggle();"> [Abstract]</a>
                        </div>
                        <div class="panel" style="display:none; font-size:18px; text-align: justify;">
Arctic sea ice can be classified as first-year (FY) or multiyear (MY) based on data collected by satellite microwave scatterometers. The Oceansat-2 Ku-band Scatterometer (OSCAT) was operational from 2009 to 2014 and is here used to classify ice as FY or MY during these years. Due to similarities in backscatter measurements from sea ice and open water, a NASA Team ice concentration product derived from passive microwave brightness temperatures is used to restrict the classification area to within the sea ice extent. Classification of FY and MY ice is completed with OSCAT by applying a temporally adjusted threshold on backscatter values. The classification method is also applied to the Quick Scatterometer (QuikSCAT) data set, and ice age classifications are processed using QuikSCAT for 1999-2009. The combined QuikSCAT and OSCAT classifications represent a 15-year record, which extends from 1999 to 2014. The classifications show a decrease in MY ice, while the total area of the ice cover remains consistent throughout winter seasons over the time series.
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="col-md-3"><a href="http://dx.doi.org/10.3390/rs8040294" target="_blank"><img width=300 src="im/ascat.png" /></a> </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong> D. B. Lindell</strong> and D. G. Long, "Multiyear Arctic Ice Classification Using ASCAT and SSMIS," in <i>Remote Sensing</i>, vol. 8, no. 4, 
                            pp. 294, Mar. 2016. doi: <a href="http://dx.doi.org/10.3390/rs8040294">10.3390/rs8040294</a>
                        </div>
                        <div style='font-size:1.5em'>
                            <a style="cursor: pointer;" class='flip' onclick="$(this).next().slideToggle();"> [Abstract]</a>
                        </div>
                        <div class="panel" style="display:none; font-size:18px; text-align: justify;">
The concentration, type, and extent of sea ice in the Arctic can be estimated based on measurements from satellite active microwave sensors, passive microwave sensors, or both. Here, data from the Advanced Scatterometer (ASCAT) and the Special Sensor Microwave Imager/Sounder (SSMIS) are employed to broadly classify Arctic sea ice type as first-year (FY) or multiyear (MY). Combining data from both active and passive sensors can improve the performance of MY and FY ice classification. The classification method uses C-band σ0 measurements from ASCAT and 37 GHz brightness temperature measurements from SSMIS to derive a probabilistic model based on a multivariate Gaussian distribution. Using a Gaussian model, a Bayesian estimator selects between FY and MY ice to classify pixels in images of Arctic sea ice. The ASCAT/SSMIS classification results are compared with classifications using the Oceansat-2 scatterometer (OSCAT), the Equal-Area Scalable Earth Grid (EASE-Grid) Sea Ice Age dataset available from the National Snow and Ice Data Center (NSIDC), and the Canadian Ice Service (CIS) charts, also available from the NSIDC. The MY ice extent of the ASCAT/SSMIS classifications demonstrates an average difference of 282 thousand km - + from that of the OSCAT classifications from 2009 to 2014. The difference is an average of 13.6% of the OSCAT MY ice extent, which averaged 2.19 million km2 over the same period. Compared to the ice classified as two years or older in the EASE-Grid Sea Ice Age dataset (EASE-2+) from 2009 to 2012, the average difference is 617 thousand km2 . The difference is an average of 22.8% of the EASE-2+ MY ice extent, which averaged 2.79 million km2 from 2009 to 2012. Comparison with the Canadian Ice Service (CIS) charts shows that most ASCAT/SSMIS classifications of MY ice correspond to a MY ice concentration of approximately 50% or greater in the CIS charts. The addition of the passive SSMIS data appears to improve classifications by mitigating misclassifications caused by ASCAT's sensitivity to rough patches of ice which can appear similar to, but are not, MY ice.
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="col-md-3"><a href="http://ieeexplore.ieee.org/document/7466093/?arnumber=7466093" target="_blank"><img width=300 src="im/soil.png" /></a> </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong>D. B. Lindell</strong> and D. G. Long, "High-Resolution Soil Moisture Retrieval With ASCAT," in 
                            <i>IEEE Geoscience and Remote Sensing Letters</i>, 
                            vol. 13, no. 7, pp. 972-976, July 2016.
                            doi: <a href="http://dx.doi.org/10.1109/LGRS.2016.2557321">10.1109/LGRS.2016.2557321</a>
                        </div>
                        <div style='font-size:1.5em'>
                            <a style="cursor: pointer;" class='flip' onclick="$(this).next().slideToggle();"> [Abstract]</a>
                        </div>
                        <div class="panel" style="display:none; font-size:18px; text-align: justify;">
Satelliteborne C-band scatterometer measurements of the radar backscatter coefficient (σ0) of the Earth can be used to estimate soil moisture levels over land. Such estimates are currently produced at 25- and 50-km resolution using the Advanced Scatterometer (ASCAT) sensor and a change detection algorithm originally developed at the Vienna University of Technology (TU-Wien). Using the ASCAT spatial response function (SRF), high-resolution (approximately 15-20 km per pixel) images of σ0 can be produced, enabling the creation of a high-resolution soil moisture product using a modified version of the TU-Wien algorithm. The high-resolution soil moisture images are compared to images produced with the Water Retrieval Package 5.5 algorithm, which is also based on the TU-Wien algorithm, and to in situ measurements from the National Oceanic and Atmospheric Administration U.S. Climate Reference Network (NOAA CRN). The WARP 5.5 and high-resolution image products generally show good agreement with each other; the high-resolution estimates appear to resolve soil moisture features at a finer scale and demonstrate a tendency toward greater moisture values in some areas. When compared to volumetric soil moisture measurements from NOAA CRN stations for 2010 and 2011, the WARP 5.5 and high-resolution soil moisture estimates perform similarly, with both having a root-mean-square difference from the in situ data of approximately 0.06 m3/m3 in one study area and 0.09 m3/m3 in another.
                        </div>

                    </td>
                </tr>
            </table>
            </div>

            <h1 id="projects">Selected Projects</h1>
            <div class="row">
            <table class="table table-hover">               
                <!--
                <tr>
                    <td class="col-md-3">
                        <div style='font-size:1.5em'>
                            <img src="im/im231n.png"
                              width="300" style="border-radius: 0px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 35px"/>
                        </div>
                    </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='text-align: justify; font-size:1.5em'>
                            <strong>Deep Networks for Robust Depth Estimation with Single-Photon Sensors</strong> <br />
                            <i>For LIDAR systems operating at extreme long range or low power, it is possible that only a few photons are detected from the laser pulse return. Using deep networks, we show that accurate depth maps can be recovered despite the challenging circumstances.</i> 
                        </div>
                   </td>
                </tr>
                --!>
                <tr>
                    <td class="col-md-3">
                        <div style='font-size:1.5em'>
                            <a href="http://stanford.edu/class/ee367/Winter2017/lindell_ee367_win17_report.pdf"> 
                                <img src="im/im367.png"
                                  width="300" style="border-radius: 0px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 35px"/>
                            </a> 
                        </div>
                    </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='text-align: justify; font-size:1.5em'>
                            <strong>Virtual Reality Motion Parallax with the Facebook Surround-360</strong> <br />
                            <i> Current virtual reality displays for viewing captured 360-degree stereo videos typically provide the wearer with a view from a single vantagepoint. We demonstrate that images from a commercial 360-degree camera rig can be processed to enable head-motion parallax while viewing with a head-mounted display. Such a viewing experience more closely mimics how we experience the real-world and can help to alleviate virtual-reality sickness and viewing discomfort.</i> 
                        </div>
                        <div style='font-size:1.5em'>
                            <a style="cursor: pointer;" class='flip' onclick="$(this).next('.panel').slideToggle();"> [Abstract]</a>
                            <a style="cursor: pointer;" href="http://stanford.edu/class/ee367/Winter2017/lindell_ee367_win17_report.pdf"> [PDF]</a>
                            <a style="cursor: pointer;" href="http://stanford.edu/class/ee367/Winter2017/lindell_ee367_win17_poster.pdf"> [Poster]</a>
                            <a style="cursor: pointer;" href="http://stanford.edu/class/ee367/Winter2017/lindell_ee367_win17_code.zip"> [Code]</a>
                        </div>
                        <div class="panel" style="display:none; font-size:18px; text-align: justify;">
                        Interest in acquiring and viewing natural scenes in full 360-
                        degree immersion has led to the development of camera rigs
                        which capture images for 360-degree stereo rendering. Most
                        such platforms capture a scene for viewing from a single vantagepoint
                        and do not incorporate motion parallax. Methods for
                        scene acquisition which do support motion parallax, such as
                        concentric mosaics or free viewpoint rendering, can be complicated
                        to implement or result in a large data format that is difficult 
                        to compress or stream. We demonstrate motion parallax
                        rendering using data acquired from the Facebook Surround-
                        360 camera rig and depth-augmented stereo panoramas. Rendered
                        views demonstrate motion parallax for horizontal head
                        motion within a range of 24 cm. We also demonstrate real-time
                        rendering of head-motion parallax from a synthetically generated
                        depth-augmented stereo panorama.
                        </div>
                    </td>
                </tr>
            </table>
            </div>

            <h1 id="experience">Experience</h1>
            <div class="row">
            <table class="table table-hover">               
                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href="http://softwareforhire.com"> 
                                <img src="im/sfh.jpg"
                                  width="200" style="border-radius: 0px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 35px"
                                  alt="Software For Hire"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 30px;"> 
                            March 2016-August-2016<br />
                            Mankato, MN (remote)
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong>Software For Hire </strong> <br />
                            <i>Position</i>: Computer Vision Specialist <br />
                            <i>Project</i>: Developed fast, multithreaded vision algorithm for a <a href="https://www.eyeconvpc.com/">pharmaceutical tablet counter </a> using open source software, including <a href="http://www.boost.org/">Boost</a>, <a href="http://opencv.org/">OpenCV</a>, and <a href="http://pointclouds.org/">Point Cloud Library</a>.
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href="http://www.rincon.com/"> 
                                <img src="im/rincon.jpg"
                                  width="200" style="border-radius: 0px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 0px"
                                  alt="Rincon Research Corporation"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 30px;"> 
                            June 2016-July-2016<br />
                            Tucson, AZ
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong>Rincon Research Corporation </strong> <br />
                            <i>Position</i>: Electrical Engineering Intern <br />
                            <i>Project</i>: Developed a cloud-based digital video recording system to stream and record live video. Integrated live broadcast television demodulation capability using <a href="http://gnuradio.org/">GNU Radio</a> and Rincon Research Corporation signal processing hardware.
                        </div>
                    </td>
                </tr>
            </table>
            </div>

            </div>

<p>Last updated on 31 Jul 2017</p>

        </div>


    <!-- jQuery library -->
    <script src="vendor/js/jquery-3.1.1.min.js"></script>
    <script src="vendor/js/bootstrap.min.js"></script>

    <!-- Smooth scrolling to specific parts of the page -->
      

    <script type="text/javascript">
        $(function() {
          $('a[href*="#"]:not([href="#"])').click(function() {
            if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'') && location.hostname == this.hostname) {
              var target = $(this.hash);
              target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
              if (target.length) {
                topMenu = $("#site_nav")
                topMenuHeight = topMenu.outerHeight() + 5
                $('html, body').animate({
                  scrollTop: target.offset().top - topMenuHeight
                }, 300);
                return false;
              }
            }
          });
        });
      </script>

    <script>
        $(document).ready(function(){
            $(".flip").click(function(){
                $(this).parent().next(".panel").slideToggle("slow");
             });
        });
    </script>


    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-85065092-1', 'auto');
      ga('send', 'pageview');

    </script>




    </body>


</html>
