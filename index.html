<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <title> David Lindell </title>
        <meta charset="utf-8"> 
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="shortcut icon" type="image/x-icon" href="/im/favicon.ico">
        <link href="https://fonts.googleapis.com/css?family=Slabo+27px|Source+Sans+Pro" rel="stylesheet">
        <link href="vendor/css/bootstrap.min.css" rel="stylesheet">
        <link href="vendor/css/bootstrap-theme.min.css" rel="stylesheet">
        <link href="vendor/css/academicons.min.css" rel="stylesheet">
        <link href="vendor/css/font-awesome.min.css" rel="stylesheet">
        <link href="css/lindell.css" rel="stylesheet">
    </head>
    <body>

      
        <nav class="navbar navbar-default navbar-fixed-top" id="site_nav">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/" style="font-size:24px">David Lindell</a>
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse" id="navbar-collapse-1">
                    <ul class="nav navbar-nav">
                        <li><a href="#news">News <span class="sr-only">(current)</span></a></li>
                        <li><a href="#education">Education </a></li>
                        <li><a href="#research">Research </a></li>
                        <li><a href="#publications">Publications </a></li>
                        <li><a href="#projects">Projects </a></li>
                        <li><a href="#experience">Experience </a></li>
                    </ul>
                    
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="http://github.com/davelindell" target="_blank">
                                <i class="fa fa-lg fa-github" aria-hidden="true"></i>
                            </a>
                        </li>
                        <li>
                            <a href="https://scholar.google.com/citations?user=_m-BTtAAAAAJ&hl=en" target="_blank">
                                <i class="ai ai-lg ai-google-scholar" aria-hidden="true"></i>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.linkedin.com/in/david-lindell-a434b882" target="_blank">
                                <i class="fa fa-lg fa-linkedin" aria-hidden="true"></i>
                            </a>
                        </li>
                    </ul>
                </div><!-- /.navbar-collapse -->

            </div><!-- /.container-fluid -->
        </nav>

        <div class = "container">
            <div class="row">
                <div class="col-md-5">
                    <h1 style='font-size: 4em; font-weight: bold; padding-bottom: 0.0em;'> David Lindell</h1>

                    <div style='font-size: 2em; font-weight: bold; padding-bottom: 0.3em;'> 
                        Ph.D. Student
                    </div>
                    <div style='font-size: 2em; font-weight: bold; padding-bottom: 0.3em;'> 
                        <a href="http://stanford.edu"> Stanford University </a>
                    </div>
                    <div style='font-size: 2em; font-weight: bold; padding-bottom: 0.3em;'> 
                        <a href="http://ee.stanford.edu"> Electrical Engineering</a>
                    </div>

                    <a href="/data/cv.pdf" target="_blank" class="btn btn-primary" style="padding: 0.5em; margin-top:-20px; margin-right:20px;">
                        <i class="fa fa-download"></i> Download CV
                    </a>
                    <a href="http://github.com/davelindell" target="_blank" style="margin:20px">
                        <i class="fa fa-3x fa-github" aria-hidden="true"></i>
                    </a>
                    <a href="https://scholar.google.com/citations?user=_m-BTtAAAAAJ&hl=en" target="_blank" style="margin:20px">
                        <i class="ai ai-3x ai-google-scholar" aria-hidden="true"></i>
                    </a>
                    <a href="https://www.linkedin.com/in/david-lindell-a434b882" target="_blank" style="margin:20px">
                        <i class="fa fa-3x fa-linkedin" aria-hidden="true"></i>
                    </a>
                </div>

                <div class="col-md-7">
                    <div style='font-size: 5em; font-weight: bold; padding-bottom: 0.3em;'> 
                        <a href="im/lindell2.jpg">
                            <img src="im/lindell2.jpg"
                              width="500" style="border-radius: 10px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 0px"
                              alt="David Lindell"/>
                        </a>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-md-11" style='font-size:1.5em'>
			I'm a fourth-year Ph.D. student at Stanford University in the <a href="http://www.computationalimaging.org/">Computational Imaging Lab </a>. My work is at the intersection of optimization, machine learning, optics, and hardware. Along these lines I've been developing next-generation computational LIDAR systems and algorithms for <a href="https://www.youtube.com/watch?v=KnGQEzB9u_0"> imaging around corners</a>. I'm generally interested in problems in 3D imaging, inverse scattering, optimization, and computer vision, with a goal of developing computational methods to push the boundaries of current imaging capabilities. My work is relevant to a broad range of applications including autonomous vehicle navigation, medical imaging, remote sensing, and robotic vision. 
                </div>  
            </div>

            <h2 id="news">News</h2>
            <div class="row">
            <table class="table table-hover">
                <tr>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                           Jan 2020
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style="font-size:1.5em;">
				My <a href="https://www.youtube.com/watch?v=ytxZc1WBtjI">TEDx talk</a> on "a camera to see around corners" is up on YouTube!
			</div>
                    </td>
                </tr>


                <tr>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                           May 2019 
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style="font-size:1.5em;">
                            Two papers accepted! <a href="http://www.computationalimaging.org/publications/acoustic-non-line-of-sight-imaging/"> Acoustic Non-Line-of-Sight Imaging </a> was accepted as an oral to CVPR, and <a href="http://www.computationalimaging.org/publications/nlos-fk/"> Wave-Based Non-Line-of-Sight Imaging Using Fast f-k Migration </a>  was accepted to SIGGRAPH.  
                        </div>
                    </td>
                </tr>

                <tr>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                           June 2018 
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style="font-size:1.5em;">
                            I'm interning at the Intelligent Systems Lab at Intel this summer with <a href="http://vladlen.info">Vladlen Koltun</a>
                        </div>
                    </td>
                </tr>

                <tr>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                           March 2018 
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em;'>
                            Our paper on seeing around corners was published in <a href="http://rdcu.be/ImAZ">Nature</a>!
                        </div>
                    </td>
                </tr>
            </table>
            </div>

            <h2 id="education">Education</h2>
            <div class="row">
            <table class="table table-hover">

                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href=""> 
                                <img src="im/su_logo.png"
                                  width="95" style="border-radius: 33px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 0px"
                                  alt="stanford"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 30px;"> 
                            2016-Present
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong> Stanford University </strong> <br />
                            Ph.D Electrical Engineering
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href="http://home.byu.edu/home/"> 
                                <img src="im/byu_logo.jpg"
                                  width="95" style="border-radius: 39px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 0px"
                                  alt="byu"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 20px;"> 
                            2009-2016
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong> Brigham Young University </strong> <br />
                            B.S. Electrical Engineering <i>Summa Cum Laude</i> <br />
                            M.S. Electrical Engineering 
                        </div>
                    </td>
                </tr>
            </table>
            </div>


            <h2 id="research">Research</h2>
            <div class="row">
            <table class="table table-hover">

                <tr>
                    <td class="col-md-5" > 
                        <div style="font-size:1.5em;"> 
                            2016-Present
                        </div>
                    </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong> Stanford University</strong>, Ph.D. Student <br />
                            <i>Advisor:</i> Gordon Wetzstein <br />
                            <i>Area:</i> computational imaging, single-photon detectors <br />
                            <i>projects:</i> 3D imaging, non-line-of-sight imaging, machine learning
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-5" > 
                        <div style="font-size:1.5em;"> 
                            2014-2016
                        </div>
                    </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong> Brigham Young University</strong>, M.S. Student <br />
                            <i>Advisor:</i> David Long <br />
                            <i>Area:</i> radar image processing, resolution enhancement, geoscience <br />
                            <i>Project:</i> Arctic ice classification, soil moisture estimation from satellite microwave sensors 
                        </div>
                    </td>
                </tr>
                
                <tr>
                    <td class="col-md-5" > 
                        <div style="font-size:1.5em;"> 
                            2013-2014
                        </div>
                    </td>
                    <td class="col-md-7 col-md-offset-0">
                        <div style='font-size:1.5em'>
                            <strong> Brigham Young University</strong>, B.S. Student <br />
                            <i>Advisor:</i> Aaron Hawkins <br />
                            <i>Area:</i> semiconductor devices, cleanroom fabrication, circuit design  <br />
                            <i>Project:</i> semi-conductor fabrication, developing solid-state single ion detectors 
                        </div>
                    </td>
                </tr>
            </table>
            </div>

            <!-- DLCT NLOS -->
            <h2 id="publications">Publications</h2>
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img height=200px src="im/cvpr2020.jpg"/>
                    <div> <h3> Non-Line-of-Sight Surface Reconstruction</br>Using the Directional Light-Cone Transform </h3> </div>
                    <div> <h3 class="unbold"> CVPR 2020</h3> </div>
                     <h4>Citation</h4> 
                    <p style='font-size:1.0em'>
                        S. I. Young, <strong>D. B. Lindell</strong>, B. Girod, D. Taubman, G. Wetzstein, “Non-line-of-sight surface reconstruction using the directional light-cone transform”, <i>Proc. CVPR</i>, 2020.
                    </p> 
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/nlos_dlct"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="http://www.computationalimaging.org/wp-content/uploads/2020/03/dlct_cvpr2020.pdf"><i class="fa fa-lg fa-file-pdf-o"></i></a>
<a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download Supplement" href="http://www.computationalimaging.org/wp-content/uploads/2020/03/dlct_supplement_cvpr2020.pdf"><i class="fa fa-lg fa-file-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Code" href="https://github.com/computational-imaging/nlos-dlct"><i class="fa fa-lg fa-code"></i></a>
                   </div>
               </div>
               <div class="panel">
               <p>We propose a joint albedo–normal approach to non-line-of-sight (NLOS) surface reconstruction using the directional light-cone transform (D-LCT). While current NLOS imaging methods reconstruct either the albedo or surface normals of the hidden scene, the two quantities provide complementary information of the scene, so an efficient method to estimate both simultaneously is desirable. We formulate the recovery of the two quantities as a vector deconvolution problem, and solve it using the Cholesky–Wiener decomposition. We show that surfaces fitted non-parametrically using our recovered normals are more accurate than those produced with NLOS surface reconstruction methods recently proposed, and are 1,000× faster to compute than using inverse rendering.</p>
              </div>
           </div>

            <!-- FK NLOS -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/siggraph2019.png"/>
                    <div> <h3> Wave-Based Non-Line-of-Sight Imaging Using Fast <i>f–k</i> Migration</h3> </div>
                    <div> <h3 class="unbold"> ACM. Trans. Graph. (SIGGRAPH) 2019</h3> </div>
                     <h4>Citation</h4> 
                    <p style='font-size:1.0em'>
                        <strong>D. B. Lindell</strong>, G. Wetzstein, M. O'Toole, “Wave-based non-line-of-sight imaging using fast f–k migration”, <i>ACM Trans. Graph. (SIGGRAPH)</i>, 38 (4), 116, 2019.
                        </p> 
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/nlos-fk"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="https://drive.google.com/a/stanford.edu/file/d/1IizX1BXRICwBEJdGNu_bFaZ6oq4E92hQ/view?usp=sharing"><i class="fa fa-lg fa-file-pdf-o"></i></a>
<a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download Supplement" href="https://drive.google.com/a/stanford.edu/file/d/1d_aY09rBeZRpup9-XxPkILbND4QESO7l/view?usp=sharing"><i class="fa fa-lg fa-file-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Code" href="https://github.com/computational-imaging/nlos-fk"><i class="fa fa-lg fa-code"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="YouTube Video" href="https://www.youtube.com/watch?v=BVYfzLXUi48"><i class="fa fa-lg fa-video-camera"></i></a>
                   </div>
                    <div style='font-size:1.5em'>
                        <p>
                        <a href="https://news.stanford.edu/2019/07/29/seeing-moving-objects-around-corners/" target="_blank"><img width=200 src="im/press/stanford-news-service-logo.png" style="margin:8px; margin-bottom:-10px;"/></a>
                        <a href="https://www.techbriefs.com/component/content/article/tb/stories/blog/34990" target="_blank"><img width=200 src="im/press/tb-logo.png" style="margin:8px; margin-top:10px; margin-bottom:-10px;"/></a>
                        <a href="https://hackaday.com/2019/08/22/looking-around-corners-with-f-k-migration/" target="_blank"><img width=180 src="im/press/hackaday-logo.png" style="margin:8px; margin-top:10px; margin-bottom:-5px;"/></a>
                        <a href="https://www.upi.com/Science_News/2019/07/30/New-camera-can-film-moving-object-from-around-a-corner/5161564503864/" target="_blank"><img width=100 src="im/press/upi-logo.png" style="margin:8px; margin-bottom:-0px; margin-top:-15px;"/></a>
                        </p>
                    </div>

               </div>

               <div class="panel">
                   <p>Imaging objects outside a camera’s direct line of sight has important applications in robotic vision, remote sensing, and many other domains. Time-of-flight-based non-line-of-sight (NLOS) imaging systems have recently demonstrated impressive results, but several challenges remain. Image formation and inversion models have been slow or limited by the types of hidden surfaces that can be imaged. Moreover, non-planar sampling surfaces and non-confocal scanning methods have not been supported by efficient NLOS algorithms. With this work, we introduce a wave-based image formation model for the problem of NLOS imaging. Inspired by inverse methods used in seismology, we adapt a frequency-domain method, f−k migration, for solving the inverse NLOS problem. Unlike existing NLOS algorithms, f−k migration is both fast and memory efficient, it is robust to specular and other complex reflectance properties, and we show how it can be used with non-confocally scanned measurements as well as for non-planar sampling surfaces. f−k migration is more robust to measurement noise than alternative methods, generally produces better quality reconstructions, and is easy to implement. We experimentally validate our algorithms with a new NLOS imaging system that records room-sized scenes outdoors under indirect sunlight, and scans persons wearing retroreflective clothing at interactive rates.</p>
               </div>
            </div>


            <!-- Acoustic NLOS -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/cvpr19.png"/>
                    <div> <h3> Acoustic Non-Line-of-Sight Imaging</h3> </div>
                    <div> <h3 class="unbold"> CVPR 2019 <i>(oral)</i> </h3> </div>
                    <h4>Citation</h4> 
                    <p style='font-size:1.0em'>
                        <strong>D. B. Lindell</strong>, G. Wetzstein, V. Koltun, “Acoustic non-line-of-sight imaging”, In <i>Proc. CVPR</i>, 2019.
                        </p> 
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/acoustic-non-line-of-sight-imaging"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="https://drive.google.com/a/stanford.edu/file/d/1nYyMLNU6XMIXkzqwxt3jEDoOap4gk_IX/view?usp=sharing"><i class="fa fa-lg fa-file-pdf-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Supplementary Material" href="https://drive.google.com/a/stanford.edu/file/d/1rkeGQwmYRgYfS_U_RcR3iPmF3tIw8LMB/view?usp=sharing"><i class="fa fa-lg fa-code"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="YouTube Video" href="https://www.youtube.com/watch?v=yoNmH3AfxCQ"><i class="fa fa-lg fa-video-camera"></i></a>
                   </div>
                    <div style='font-size:1.5em'>
                        <p>
                        <a href="https://www.sciencemag.org/news/2019/06/scientists-use-sound-see-around-corners" target="_blank"><img width=200 src="im/press/science.png" style="margin:8px; margin-bottom:-10px;"/></a>
                        <a href="https://venturebeat.com/2019/06/17/intel-highlights-ai-that-can-see-around-corners-coach-children-on-the-autism-spectrum-and-more-during-cvpr/" target="_blank"><img width=200 src="im/press/venturebeat.png" style="margin:8px; margin-top:30px; margin-bottom:-10px;"/></a>
                        <a href="https://gizmodo.com/researchers-follow-bats-example-use-cheap-speakers-and-1835581284" target="_blank"><img width=200 src="im/press/gizmodo.png" style="margin:8px; margin-bottom:-10px; margin-top:10px;"/></a>
                        </p>
                    </div>

               </div>

               <div class="panel">
                  <p>Non-line-of-sight (NLOS) imaging enables unprecedented capabilities in a wide range of applications, including robotic and machine vision, remote sensing, autonomous vehicle navigation, and medical imaging. Recent approaches to solving this challenging problem employ optical time-of-flight imaging systems with highly sensitive time-resolved photodetectors and ultra-fast pulsed lasers. However, despite recent successes in NLOS imaging using these systems, widespread implementation and adoption of the technology remains a challenge because of the requirement for specialized, expensive hardware. We introduce acoustic NLOS imaging, which is orders of magnitude less expensive than most optical systems and captures hidden 3D geometry at longer ranges with shorter acquisition times compared to state-of-the-art optical methods. Inspired by hardware setups used in radar and algorithmic approaches to model and invert wave-based image formation models developed in the seismic imaging community, we demonstrate a new approach to seeing around corners.</p>
               </div>
            </div>

            <!-- Occlusion NLOS -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/tog2019.jpg"/>
                    <div> <h3> Non-Line-of-Sight Imaging with Partial Occluders and Surface Normals</h3> </div>
                    <div> <h3 class="unbold"> ACM Trans. Graph. 2019 </h3> </div>
                    <h4>Citation</h4>
                    <p style='font-size:1.0em'>
                        F. Heide, M. O'Toole, K. Zang, <strong>D. B. Lindell</strong>, S. Diamond, G. Wetzstein, “Non-line-of-sight imaging with partial occluders and surface normals”, <i>ACM Trans. Graph.</i>, 2019.</p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/non-line-of-sight-imaging-with-partial-occluders-and-surface-normals/"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="https://drive.google.com/file/d/1JpuUYYrNvTlYyunvsVrBJZRdDJ5SVEed/view"> <i class="fa fa-lg fa-file-pdf-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Supplementary Material" href="https://drive.google.com/file/d/1Jq9-47B44eTJwLVwE0gweU1nw6lqB1kz/view"><i class="fa fa-lg fa-code"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="YouTube Video" href="https://www.youtube.com/watch?v=nVoEZuFFMzA"><i class="fa fa-lg fa-video-camera"></i></a>
                    </div>
               </div>
               <div class="panel">
                  <p>Imaging objects obscured by occluders is a significant challenge for many applications. A camera that could “see around corners” could help improve navigation and mapping capabilities of autonomous vehicles or make search and rescue missions more effective. Time-resolved single-photon imaging systems have recently been demonstrated to record optical information of a scene that can lead to an estimation of the shape and reflectance of objects hidden from the line of sight of a camera. However, existing non-line-of-sight (NLOS) reconstruction algorithms have been constrained in the types of light transport effects they model for the hidden scene parts. We introduce a factored NLOS light transport representation that accounts for partial occlusions and surface normals. Based on this model, we develop a factorization approach for inverse time-resolved light transport and demonstrate high-fidelity NLOS reconstructions for challenging scenes both in simulation and with an experimental NLOS imaging system.</p>
               </div>
            </div>
            
            <!-- Pileup -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/scientificreports2018.png"/>
                    <div><h3> Sub-Picosecond Photon-Efficient 3D Imaging</br>Using Single-Photon Sensors</h3></div>
                    <div><h3 class="unbold"> Scientific Reports 2018 </h3></div>
                    <h4>Citation</h4>
                    <p style='display:block; font-size:1.0em'>
                        F. Heide, S. Diamond, <strong>D. B. Lindell</strong>, G. Wetzstein, “Sub-picosecond photon-efficient 3D imaging using single-photon sensors”, <i>Scientific Reports</i>, 17726, 2018.
                    </p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/sub-picosecond-photon-efficient-3d-imaging-using-single-photon-sensors/"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="https://www.nature.com/articles/s41598-018-35212-x"> <i class="fa fa-lg fa-file-pdf-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Supplementary Material" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-018-35212-x/MediaObjects/41598_2018_35212_MOESM1_ESM.pdf"><i class="fa fa-lg fa-code"></i></a>
                    </div>
               </div>
               <div class="panel">
                  <p>Active 3D imaging systems have broad applications across disciplines, including biological imaging, remote sensing and robotics. Applications in these domains require fast acquisition times, high timing accuracy, and high detection sensitivity. Single-photon avalanche diodes (SPADs) have emerged as one of the most promising detector technologies to achieve all of these requirements. However, these detectors are plagued by measurement distortions known as pileup, which fundamentally limit their precision. In this work, we develop a probabilistic image formation model that accurately models pileup. We devise inverse methods to efficiently and robustly estimate scene depth and reflectance from recorded photon counts using the proposed model along with statistical priors. With this algorithm, we not only demonstrate improvements to timing accuracy by more than an order of magnitude compared to the state-of-the-art, but our approach is also the first to facilitate sub-picosecond-accurate, photon-efficient 3D imaging in practical scenarios where widely-varying photon counts are observed.</p>
               </div>
            </div>

            <!-- SIGGRAPH 2018 -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/siggraph_2018.jpg"/>
                    <div><h3> Single-Photon 3D Imaging with Deep Sensor Fusion</h3></div>
                    <div><h3 class="unbold"> ACM Trans. Graph. (SIGGRAPH) 2018 </h3></div>
                    <h4>Citation</h4>
                    <p style='font-size:1.0em'>
                        <strong>D. B. Lindell</strong>, M. O'Toole, G. Wetzstein, “Single-photon 3D imaging with deep sensor fusion”, <i>ACM Trans. Graph. (SIGGRAPH)</i>, 2018.
                    </p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/single-photon-3d-imaging-with-deep-sensor-fusion/"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="https://drive.google.com/file/d/1eTIawad7GFob3p5nuxrDcO7FMtL3AzSt/view"><i class="fa fa-lg fa-file-pdf-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Supplementary Material" href="https://drive.google.com/file/d/1DtFQKKaPoP4Btj1Nv6DowuWC_JBBa-cT/view"><i class="fa fa-lg fa-code"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="YouTube Video" href="https://www.youtube.com/watch?v=dg_73m4e_Js"><i class="fa fa-lg fa-video-camera"></i></a>
                    </div>
               </div>
               <div class="panel">
                  <p>Sensors which capture 3D scene information provide useful data for tasks in vehicle navigation, gesture recognition, human pose estimation, and geometric reconstruction. Active illumination time-of-flight sensors in particular have become widely used to estimate a 3D representation of a scene. However, the maximum range, density of acquired spatial samples, or overall acquisition time of these sensors is fundamentally limited by the minimum signal required to estimate depth reliably. In this paper, we propose a data-driven method for photon-efficient 3D imaging which leverages sensor fusion and computational reconstruction to rapidly and robustly estimate a dense depth map from low photon counts. Our sensor fusion approach uses measurements of single photon arrival times from a low-resolution single-photon detector array and an intensity image from a conventional high-resolution camera. Using a multi-scale deep convolutional network, we jointly process the raw measurements from both sensors and output a high-resolution depth map. To demonstrate the efficacy of our approach, we implement a hardware prototype and show results using captured data. At low signal-to-background levels our depth reconstruction algorithm with sensor fusion outperforms other methods for depth estimation from noisy measurements of photon arrival times.</p>
               </div>
            </div>

            <!-- ICCP 2018 -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/iccp_2018.png"/>
                    <div><h3>Towards Transient Imaging at Interactive Rates</br>with Single-Photon Detectors </h3></div>
                    <div><h3 class="unbold"> ICCP 2018 </h3></div>
                    <h4>Citation</h4>
                    <p style='font-size:1.0em'>
                        <strong>D. B. Lindell</strong>, M. O'Toole, G. Wetzstein, “Towards transient imaging at interactive rates with single-photon detectors”, In <i>Proc. ICCP</i>, 2018.
                    </p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/reconstructing-transient-images-from-single-photon-sensors-cvpr-2017/"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="https://drive.google.com/a/stanford.edu/file/d/1b2Ls8bUghMl2h8gZMnhgfSYbbRHb83RD/view?usp=sharing"><i class="fa fa-lg fa-file-pdf-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Conference Presentation" href="https://www.youtube.com/watch?time_continue=2607&v=pgZ3HiNaF4k"><i class="fa fa-lg fa-user"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="YouTube Video" href="https://www.youtube.com/watch?v=i6Z3NDgXLec"><i class="fa fa-lg fa-video-camera"></i></a>
                    </div>
               </div>
               <div class="panel">
                  <p> Active imaging at the picosecond timescale reveals transient light transport effects otherwise not accessible by com- puter vision and image processing algorithms. For example, analyzing the time of flight of short laser pulses emitted into a scene and scattered back to a detector allows for depth imaging, which is crucial for autonomous driving and many other applications. Moreover, analyzing or removing global light transport effects from photographs becomes feasible.  While several transient imaging systems have recently been proposed using various imaging technologies, none is capable of acquiring transient images at interactive frame rates. In this paper, we present an imaging system that leverages single-photon avalanche diodes together with a pulsed picosecond laser to record transient images with up to 25 Hz at a low spatial resolution of 64 x 80 pixels or 1 Hz at a moderate resolution of 256 x 250 pixels. We show several transient video clips recorded with this system and demonstrate transient imaging applications, including direct-global light transport separation and enhanced depth imaging.</p>
               </div>
            </div>

            <!-- Nature 2018 -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/nature_teaser.png"/>
                    <div><h3>Confocal Non-Line-of-Sight Imaging based on the Light-Cone Transform </h3></div>
                    <div><h3 class="unbold"> Nature 2018 </h3></div>
                    <h4>Citation</h4>
                    <p style='font-size:1.0em'>
                        <strong> M. O'Toole, D. B. Lindell</strong>, G. Wetzstein, “Confocal non-line-of-sight imaging based on the light-cone transform”, <i>Nature</i>, 555 (7696), 338, 2018.
                    </p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/confocal-non-line-of-sight-imaging-based-on-the-light-cone-transform/"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="http://rdcu.be/ImAZ"><i class="fa fa-lg fa-file-pdf-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Supplementary Material" href="https://www.nature.com/articles/nature25489#supplementary-information"><i class="fa fa-lg fa-code"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="YouTube Video" href="https://www.youtube.com/watch?v=lCJN_RwJPew"><i class="fa fa-lg fa-video-camera"></i></a>
                    </div>
                    <div style='font-size:1.5em'>
                        <p>
                        <a href="https://www.youtube.com/watch?v=KnGQEzB9u_0" target="_blank"><img width=200 src="im/press/stanford-news-service-logo.png" style="margin:8px; margin-bottom:-10px;"/></a>
                        <a href="https://www.wired.com/story/wanna-see-around-corners-better-get-yourself-a-laser/" target="_blank"><img width=200 src="im/press/wired.png" style="margin:8px; margin-bottom:-10px;"/></a>
                        <a href="https://www.theguardian.com/technology/2018/mar/05/self-driving-cars-may-soon-be-able-to-see-around-corners" target="_blank"><img width=200 src="im/press/guardian.png" style="margin:8px; margin-bottom:-10px; margin-top:-12px;"/></a>
                        </p>
                    </div>
               </div>
               <div class="panel">
                  <p>Imaging objects hidden from a camera’s view is a problem of fundamental importance to many fields of research with applications in robotic vision, defense, remote sensing, medical imaging, and autonomous vehicles. Non-line-of-sight (NLOS) imaging at macroscopic scales has been demonstrated by scanning a visible surface with a pulsed laser and time-resolved detector. Whereas light detection and ranging (LIDAR) systems use such measurements to recover the shape of visible objects from direct reflections, NLOS imaging aims at reconstructing the shape and albedo of hidden objects from multiply scattered light. Despite recent advances, NLOS imaging has remained impractical due to the prohibitive memory and processing requirements of existing reconstruction algorithms, and the extremely weak signal of multiply scattered light. Here we show that confocalizing the scanning procedure provides a means to address these key challenges. Confocal scanning facilitates the derivation of a novel closed-form solution to the NLOS reconstruction problem, which requires computational and memory resources that are orders of magnitude fewer than previous reconstruction methods and recovers hidden objects at unprecedented image resolutions. Confocal scanning also uniquely benefits from a sizeable increase in signal and range when imaging retroreflective objects. We quantify the resolution bounds of NLOS imaging, demonstrate real-time tracking capabilities, and derive efficient algorithms that incorporate image priors and a physically-accurate noise model. Most notably, we demonstrate successful outdoor experiments for NLOS imaging under indirect sunlight.</p>
               </div>
            </div>

            <!-- CVPR 2017 -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/cvpr17.png"/>
                    <div><h3> Reconstructing Transient Images from Single-Photon Sensors</h3></div>
                    <div><h3 class="unbold"> CVPR 2017 <i>(spotlight)</i></h3></div>
                    <h4>Citation</h4>
                    <p style='font-size:1.0em'>
                        M. O'Toole, F. Heide, <strong>D. B. Lindell</strong>, K.Zang, S. Diamond, G. Wetzstein, “Reconstructing transient images from single-photon sensors”, In <i>Proc. CVPR</i>, 2017.
                    </p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="http://www.computationalimaging.org/publications/reconstructing-transient-images-from-single-photon-sensors-cvpr-2017/"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="http://www.computationalimaging.org/wp-content/uploads/2017/05/17.cvpr_.spad_.pdf"><i class="fa fa-lg fa-file-pdf-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Supplementary Material" href="http://www.computationalimaging.org/wp-content/uploads/2017/08/17.cvpr_.spad_.supp_.pdf"><i class="fa fa-lg fa-code"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="YouTube Video" href="https://www.youtube.com/watch?v=kNI8JL32Neo"><i class="fa fa-lg fa-video-camera"></i></a>
                    </div>
               </div>
               <div class="panel">
                            <p>Computer vision algorithms build on 2D images or 3D videos that capture dynamic events at the millisecond time scale. However, capturing and analyzing “transient images” at the picosecond scale—i.e., at one trillion frames per second—reveals unprecedented information about a scene and light transport within. This is not only crucial for time-of-flight range imaging, but it also helps further our understanding of light transport phenomena at a more fundamental level and potentially allows to revisit many assumptions made in different computer vision algorithms.</p>

                            <p>In this work, we design and evaluate an imaging system that builds on single photon avalanche diode (SPAD) sensors to capture multi-path responses with picosecond-scale active illumination. We develop inverse methods that use modern approaches to deconvolve and denoise measurements in the presence of Poisson noise, and compute transient images at a higher quality than previously reported. The small form factor, fast acquisition rates, and relatively low cost of our system potentially makes transient imaging more practical for a range of applications.</p>
               </div>
            </div>

            <!-- GRSL 2016 -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/soil.png"/>
                    <div><h3> High-Resolution Soil Moisture Retrieval with ASCAT</h3></div>
                    <div><h3 class="unbold"> IEEE Geoscience and Remote Sensing Letters 2016</h3></div>
                    <h4>Citation</h4>
                    <p style='font-size:1.0em'>
                         <strong>D. B. Lindell</strong>, D. Long, “High-resolution soil moisture retrieval with ASCAT”, <i>Geoscience and Remote Sensing Letters</i>, 13 (7), 972-976, 2016.
                    </p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="https://ieeexplore.ieee.org/document/7466093"><i class="fa fa-lg fa-external-link"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Code" href="https://github.com/davelindell/soil_moisture"><i class="fa fa-lg fa-code"></i></a>
                    </div>
               </div>
               <div class="panel">
                   <p>Satelliteborne C-band scatterometer measurements of the radar backscatter coefficient (σ0) of the Earth can be used to estimate soil moisture levels over land. Such estimates are currently produced at 25- and 50-km resolution using the Advanced Scatterometer (ASCAT) sensor and a change detection algorithm originally developed at the Vienna University of Technology (TU-Wien). Using the ASCAT spatial response function (SRF), high-resolution (approximately 15-20 km per pixel) images of σ0 can be produced, enabling the creation of a high-resolution soil moisture product using a modified version of the TU-Wien algorithm. The high-resolution soil moisture images are compared to images produced with the Water Retrieval Package 5.5 algorithm, which is also based on the TU-Wien algorithm, and to in situ measurements from the National Oceanic and Atmospheric Administration U.S. Climate Reference Network (NOAA CRN). The WARP 5.5 and high-resolution image products generally show good agreement with each other; the high-resolution estimates appear to resolve soil moisture features at a finer scale and demonstrate a tendency toward greater moisture values in some areas. When compared to volumetric soil moisture measurements from NOAA CRN stations for 2010 and 2011, the WARP 5.5 and high-resolution soil moisture estimates perform similarly, with both having a root-mean-square difference from the in situ data of approximately 0.06 m3/m3 in one study area and 0.09 m3/m3 in another.
</p>
               </div>
            </div>

            <!-- Remote Sensing 2016 -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/ascat.png">
                    <div><h3> Multiyear Arctic Ice Classification Using ASCAT and SSMIS</h3></div>
                    <div><h3 class="unbold"> Remote Sensing 2016</h3></div>
                    <h4>Citation</h4>
                    <p style='font-size:1.0em'>
                         <strong>D. B. Lindell</strong>, D. Long, “Multiyear Arctic Ice Classification Using ASCAT and SSMIS”, <i>Remote Sensing</i>, 8 (4), 294, 2016.
                    </p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="https://www.mdpi.com/2072-4292/8/4/294"><i class="fa fa-lg fa-external-link"></i></a>
                    </div>
               </div>
               <div class="panel">
                   <p>The concentration, type, and extent of sea ice in the Arctic can be estimated based on measurements from satellite active microwave sensors, passive microwave sensors, or both. Here, data from the Advanced Scatterometer (ASCAT) and the Special Sensor Microwave Imager/Sounder (SSMIS) are employed to broadly classify Arctic sea ice type as first-year (FY) or multiyear (MY). Combining data from both active and passive sensors can improve the performance of MY and FY ice classification. The classification method uses C-band σ0 measurements from ASCAT and 37 GHz brightness temperature measurements from SSMIS to derive a probabilistic model based on a multivariate Gaussian distribution. Using a Gaussian model, a Bayesian estimator selects between FY and MY ice to classify pixels in images of Arctic sea ice. The ASCAT/SSMIS classification results are compared with classifications using the Oceansat-2 scatterometer (OSCAT), the Equal-Area Scalable Earth Grid (EASE-Grid) Sea Ice Age dataset available from the National Snow and Ice Data Center (NSIDC), and the Canadian Ice Service (CIS) charts, also available from the NSIDC. The MY ice extent of the ASCAT/SSMIS classifications demonstrates an average difference of 282 thousand km - + from that of the OSCAT classifications from 2009 to 2014. The difference is an average of 13.6% of the OSCAT MY ice extent, which averaged 2.19 million km2 over the same period. Compared to the ice classified as two years or older in the EASE-Grid Sea Ice Age dataset (EASE-2+) from 2009 to 2012, the average difference is 617 thousand km2 . The difference is an average of 22.8% of the EASE-2+ MY ice extent, which averaged 2.79 million km2 from 2009 to 2012. Comparison with the Canadian Ice Service (CIS) charts shows that most ASCAT/SSMIS classifications of MY ice correspond to a MY ice concentration of approximately 50% or greater in the CIS charts. The addition of the passive SSMIS data appears to improve classifications by mitigating misclassifications caused by ASCAT's sensitivity to rough patches of ice which can appear similar to, but are not, MY ice.</p>
               </div>
            </div>

            <!-- TGRS 2016 -->
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/oscat.png"/>
                    <div><h3> Multiyear Arctic Ice Classification Using OSCAT and QuikSCAT</h3></div>
                    <div><h3 class="unbold"> IEEE Transactions on Geoscience and Remote Sensing 2016</h3></div>
                    <h4>Citation</h4>
                    <p style='font-size:1.0em'>
                         <strong>D. B. Lindell</strong>, D. Long, “Multiyear Arctic Ice Classification Using OSCAT and QuikSCAT”, <i>IEEE Transactions on Geoscience and Remote Sensing</i>, 54 (1), 167-175, 2016.
                    </p>
                    <div class="links" style='font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Project Webpage" href="https://ieeexplore.ieee.org/document/7172497/"><i class="fa fa-lg fa-external-link"></i></a>
                    </div>
               </div>
               <div class="panel" >
                   <p>Arctic sea ice can be classified as first-year (FY) or multiyear (MY) based on data collected by satellite microwave scatterometers. The Oceansat-2 Ku-band Scatterometer (OSCAT) was operational from 2009 to 2014 and is here used to classify ice as FY or MY during these years. Due to similarities in backscatter measurements from sea ice and open water, a NASA Team ice concentration product derived from passive microwave brightness temperatures is used to restrict the classification area to within the sea ice extent. Classification of FY and MY ice is completed with OSCAT by applying a temporally adjusted threshold on backscatter values. The classification method is also applied to the Quick Scatterometer (QuikSCAT) data set, and ice age classifications are processed using QuikSCAT for 1999-2009. The combined QuikSCAT and OSCAT classifications represent a 15-year record, which extends from 1999 to 2014. The classifications show a decrease in MY ice, while the total area of the ice cover remains consistent throughout winter seasons over the time series.</p>
               </div>
            </div>

            <h2 id="projects">Selected Projects</h2>
            <div class="publication pub-collapsed">
                <div class="pubwrap">
                    <img width=300px src="im/im367.png"/>
                    <div><h3>Virtual Reality Motion Parallax with the Facebook Surround-360</h3></div>
                    <p style='font-size:1.0em'>
                        D. B. Lindell, J. Thatte, EE 367, 2017.
                    </p>
                    <p style="font-size:1.2em">Current virtual reality displays for viewing captured 360-degree stereo videos typically provide the wearer with a view from a single vantagepoint. We demonstrate that images from a commercial 360-degree camera rig can be processed to enable head-motion parallax while viewing with a head-mounted display. Such a viewing experience more closely mimics how we experience the real-world and can help to alleviate virtual-reality sickness and viewing discomfort. </p>
                    <div class="links" style='margin-top:20px; font-size:1.5em'>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Download PDF" href="http://stanford.edu/class/ee367/Winter2017/lindell_ee367_win17_report.pdf"><i class="fa fa-lg fa-file-pdf-o"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="Code" href="http://stanford.edu/class/ee367/Winter2017/lindell_ee367_win17_code.zip"><i class="fa fa-lg fa-code"></i></a>
                        <a style="cursor: pointer;" data-toggle="tooltip" target="_blank" title="poster" href="http://stanford.edu/class/ee367/Winter2017/lindell_ee367_win17_poster.pdf"><i class="fa fa-lg fa-file-o"></i></a>
                    </div>

               </div>
            </div>

            <h2 id="experience">Experience</h2>
            <div class="row">
            <table class="table table-hover">               
                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href="http://intel.com"> 
                                <img src="im/intel.png"
                                  width="200" style="border-radius: 0px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 10px"
                                  alt="Intel"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 30px;"> 
                            June 2018-Oct 2018<br />
                            Santa Clara, CA
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong>Intel Intelligent Systems Lab</strong> <br />
                            <i>Position</i>: Intern <br />
                            <i>Project</i>: Developed hardware system and algorithms for acoustic non-line-of-sight imaging.
                        </div>
                    </td>
                </tr>
                <!--<tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <img src="im/dahlia.png"
                              width="200" style="border-radius: 0px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 35px"
                              alt="Dahlia Lights"/>
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 30px;"> 
                            September 2017-January 2019<br />
                            Palo Alto, CA 
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong>Dahlia Lighting (startup) </strong> <br />
                            <i>Position</i>: Computer Vision Specialist <br />
                            <i>Project</i>: Worked on computer vision for smart lighting systems. Acquired by <a href="https://lexidevices.com/">Lexi Devices</a>.
                        </div>
                    </td> 
                </tr>-->

                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href="http://softwareforhire.com"> 
                                <img src="im/sfh.jpg"
                                  width="200" style="border-radius: 0px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 35px"
                                  alt="Software For Hire"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 30px;"> 
                            March 2016-June 2019<br />
                            Mankato, MN (remote)
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong>Software For Hire </strong> <br />
                            <i>Position</i>: Computer Vision Consultant <br />
                            <i>Project</i>: Developed fast, multithreaded vision algorithm for a <a href="https://www.eyeconvpc.com/">pharmaceutical tablet counter </a> using <a href="http://www.boost.org/">Boost</a>, <a href="http://opencv.org/">OpenCV</a>, and <a href="http://pointclouds.org/">Point Cloud Library</a>. Built and shipped an improved neural-net-based algorithm for recognition and counting using <a href="http://pytorch.org">PyTorch</a> and <a href="http://mxnet.apache.org">MXNet</a> for real-time inference on Intel CPUs.
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="col-md-1"> 
                        <div style='font-size:1.5em'>
                            <a href="http://www.rincon.com/"> 
                                <img src="im/rincon.jpg"
                                  width="200" style="border-radius: 0px; margin: 10px; max-width: none; margin-left: 0px; margin-top: 0px"
                                  alt="Rincon Research Corporation"/>
                            </a> 
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em; margin-top: 30px;"> 
                            June 2016-July 2016<br />
                            Tucson, AZ
                        </div>
                    </td>
                    <td class="col-md-7">
                        <div style='font-size:1.5em'>
                            <strong>Rincon Research Corporation </strong> <br />
                            <i>Position</i>: Electrical Engineering Intern <br />
                            <i>Project</i>: Developed a cloud-based digital video recording system to stream and record live video. Integrated live broadcast television demodulation capability using <a href="http://gnuradio.org/">GNU Radio</a> and proprietary signal processing hardware.
                        </div>
                    </td>
                </tr>
            </table>
            </div>


            <h2 id="talks">Invited Talks</h2>
            <div class="row">
            <table class="table table-hover">               
                <tr>
                    <td class="col-md-4"> 
                        <div style='font-size:1.5em'>
                            <strong> TEDxBeaconStreet 2019 </strong>
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                            <i>How future cars will see around corners</i>
                        </div>
                    </td>
                    <td class="col-md-2">
                        <div style="font-size: 1.5em;"> 
                        11/23/2019
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-4"> 
                        <div style='font-size:1.5em'>
                            <strong> Boston University Center for Information & Systems Engineering </strong>
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                            <i>Computational Imaging with Single-Photon Detectors</i>
                        </div>
                    </td>
                    <td class="col-md-2">
                        <div style="font-size: 1.5em;"> 
                        11/22/2019
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-4"> 
                        <div style='font-size:1.5em'>
                            <strong> MIT Research Laboratory of Electronics </strong>
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                            <i>Efficient Confocal Non-Line-of-Sight Imaging</i>
                        </div>
                    </td>
                    <td class="col-md-2">
                        <div style="font-size: 1.5em;"> 
                        11/21/2019
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-4"> 
                        <div style='font-size:1.5em'>
                            <strong> Berkeley Center for Computational Imaging </strong>
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                            <i>Computational Imaging with Single-Photon Detectors</i>
                        </div>
                    </td>
                    <td class="col-md-2">
                        <div style="font-size: 1.5em;"> 
                        9/4/2019
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-4"> 
                        <div style='font-size:1.5em'>
                            <strong> Silicon Valley ACM SIGGRAPH Chapter </strong>
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                            <i>Computational Single-Photon Imaging</i>
                        </div>
                    </td>
                    <td class="col-md-2">
                        <div style="font-size: 1.5em;"> 
                        5/30/2019
                        </div>
                    </td>
                </tr>

                <tr>
                    <td class="col-md-4"> 
                        <div style='font-size:1.5em'>
                            <strong> Stanford Center for Image Systems Engineering </strong>
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                            <i>Computational Imaging with Single-Photon Detectors</i>
                        </div>
                    </td>
                    <td class="col-md-2">
                        <div style="font-size: 1.5em;"> 
                        5/8/2019
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="col-md-4"> 
                        <div style='font-size:1.5em'>
                            <strong> Carnegie Mellon University Graphics Lab </strong>
                        </div>
                    </td>
                    <td  class="col-md-4">
                        <div style="font-size:1.5em;"> 
                            <i>Computational Single-Photon Imaging</i>
                        </div>
                    </td>
                    <td class="col-md-2">
                        <div style="font-size: 1.5em;"> 
                        1/23/2019
                        </div>
                    </td>
                </tr>
           </table>
           </div>
           </div>


<p>Last updated on 17 Mar 2020</p>

        </div>


    <!-- jQuery library -->
    <script src="vendor/js/jquery-3.1.1.min.js"></script>
    <script src="vendor/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="scripts.js"></script>

    </body>


</html>
